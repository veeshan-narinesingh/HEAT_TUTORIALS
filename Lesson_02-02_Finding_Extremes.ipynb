{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f603b-4d10-4af2-bde3-c72c21b38842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will take things a step further and start identifying extremes\n",
    "# It turns out, there are different ways of calculating an anomaly\n",
    "# For our purposes of identifying persistent heat extremes, we will follow the methodology\n",
    "# in the Chan et al. 2019 paper: https://doi.org/10.1038/s41612-022-00290-2\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import pandas as pd\n",
    "\n",
    "# inputs\n",
    "tdir = 'DATA/'\n",
    "tfileprefix = 'ERA5_2deg_temperature.model_level_137.daily.'\n",
    "\n",
    "ny1=1979 # specify the first year of the data files\n",
    "ny2=2019 # specify the last year of the data files\n",
    "\n",
    "\n",
    "# specify the location of focus\n",
    "locname='NYC' #specify a name for the location you are looking at\n",
    "lonwant=360-74 # NY is 74 West longitude, but the longitude dimension goes from 0 to 360, so we do 360-74\n",
    "latwant=40.71\n",
    "\n",
    "savename= locname +'_hwstardates_and_durations.nc' #specify a savename for your file\n",
    "print(\"Savename: \" +savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add9e398-3a77-4e40-a824-7a5ee02af760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the temperature data for each year \n",
    "nyrs=ny2-ny1+1\n",
    "\n",
    "time = np.zeros([365 * nyrs])\n",
    "c1 = 0\n",
    "c2 = 365\n",
    "\n",
    "for yr in range(ny1,ny2 + 1):\n",
    "    filenamet = tfileprefix + str(yr) + '.nc'\n",
    "    ds = xr.open_dataset(tdir + filenamet)\n",
    "\n",
    "    if yr == ny1:\n",
    "        lat = ds.lat\n",
    "        lon = ds.lon\n",
    "        # preallocate t2m as an xarray\n",
    "        var1np = np.zeros([365 * nyrs,np.size(lat),np.size(lon)])\n",
    "        ratime = np.zeros([365 * nyrs])\n",
    "        \n",
    "    var1np[c1:c2,:,:] = ds.t[:365,:,:].values\n",
    "    time[c1:c2] = np.arange(0,365,1) + 365 * (yr - ny1)\n",
    "    ratime[c1:c2] = ds.time[:365]\n",
    "    c1 = 1 * c2\n",
    "    c2 = c2 + 365\n",
    "var1 = xr.DataArray(data=var1np,dims=['time','lat','lon'],coords=dict(time=time, lat=lat, lon=lon))\n",
    "varloc = var1.sel(lat=latwant,lon=lonwant,method='nearest') \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47dcc6-f41c-4ab8-a740-6ac5ef33f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First step in Chan's Algorithm is to do a 5-day smooth on the data to isolate persistent events\n",
    "var1smooth5=varloc.rolling(time=5,center=True,min_periods=1).mean()#5 day smooth using running mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b42a7a-270d-4026-9156-ae5992a53d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's a task to test your understanding. Make a plot of the raw a.k.a. full temperature field (we have done this in previous lesson)\n",
    "#Then plot the 5 day smoothed field on the same figure.\n",
    "#Describe what you see when comparing the two plots. Add a legend to your graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5d709-412b-432d-a711-5e8fe223da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "var1smooth5=varloc.rolling(time=5,center=True,min_periods=1).mean()#5 day smooth using running mean\n",
    "# turn into array and reshape to be # of  years by 365 days\n",
    "var1smooth5np=var1smooth5.values.reshape((nyrs,365))\n",
    "#now turn back into xarray\n",
    "var1smooth5rs= xr.DataArray(data=var1smooth5np,dims=[\"year\",\"day\"],coords=dict(year=np.arange(ny1,ny2+1,1),day=np.arange(1,366,1)))\n",
    "var1smooth5rs15day1=var1smooth5rs.rolling(day=15,center=True,min_periods=1).mean()#1st 15 day running mean \n",
    "var1smooth5rs15day2=var1smooth5rs15day1.rolling(day=15,center=True,min_periods=1).mean()#2nd 15 day running mean \n",
    "clim=var1smooth5rs15day2.rolling(year=11,center=True,min_periods=1).mean()#29 day by 11 year climatology\n",
    "anom=var1smooth5rs-clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f193db9-d26b-4c5c-88d3-61f92b3d7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot a histogram to get a sense of how these temperature anomalies are distributed\n",
    "plt.hist(anom.values.flatten(),bins='auto')\n",
    "plt.xlabel('Temperature Anomaly (K)')\n",
    "plt.ylabel('Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ee0ea7-3d91-4eed-bff1-0cad035d1a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find 90th percentile for summer heat\n",
    "anomjja=anom[:,151:242] #june 1st is idx 151, since it is the 152nd day of the year, Aug 31 is idx 242\n",
    "thresh90=np.percentile((anomjja),90)\n",
    "thresh90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a3bbe-c9e0-41b2-abf0-12938b5dab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks for you: \n",
    "\n",
    "#1. Calculate the 10th percentile.\n",
    "#2. Calculate the 50th percentile.\n",
    "#3. Calculate the 99th percentile.\n",
    "#4. Calculate the 1st percentile.\n",
    "\n",
    "#do your answers make sense according to the histogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e9ff4-dbb2-4282-ace1-02c432ca3727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The rest of the code will now find and track the heat extremes. Basically it looks for all points in time that the temperature\n",
    "#anomaly is above the 90th percentile threshold. Don't worry too much about this part, think of it as a black box.\n",
    "\n",
    "#get hw dates at location \n",
    "timers=time.reshape((nyrs,365))\n",
    "timersjja=timers[:,150:242] \n",
    "\n",
    "anomjjamat=anomjja.values\n",
    "\n",
    "hwidcs=np.where(anomjjamat>thresh90)\n",
    "\n",
    "\n",
    "hwdates=timersjja[hwidcs]\n",
    "\n",
    "#find start dates\n",
    "test = 2.0 * hwdates / hwdates\n",
    "test[1:] = np.diff(hwdates)\n",
    "independent_events = np.where(test != 1.0, 1.0, 0.0)\n",
    "event_indices = independent_events * hwdates\n",
    "event_startdates = event_indices[event_indices != 0].astype('int')\n",
    "\n",
    "hwdatesint=hwdates.astype(int)\n",
    "bimat = np.zeros(int(max(hwdates)))\n",
    "for id in hwdatesint:\n",
    "    bimat[int(id-1)]=1\n",
    "    \n",
    "from scipy.ndimage import label\n",
    "labels, num_components = label(bimat)\n",
    "print(\"Labels:\")\n",
    "print(labels)\n",
    "print(\"Number of components:\", num_components)\n",
    "\n",
    "# Calculate the size of each label\n",
    "label_sizes = np.bincount(labels.ravel())\n",
    "\n",
    "durations=label_sizes[1:]       \n",
    "\n",
    "#save start dates and durations\n",
    "event_startdates_xr = xr.DataArray(data=event_startdates).rename('event_startdates')\n",
    "durations_xr = xr.DataArray(data=durations).rename('event_duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffd402-f5e7-46c0-b247-bcda0ff3994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the average heat extreme duration and make a histogram\n",
    "print('Average Duration: ' + str(durations.mean()) + ' days')\n",
    "\n",
    "plt.hist(durations,bins=list(range(1, 16))) #bins=list(range(1, 16)) produces histogram bins that go to 1 to 15\n",
    "plt.xlabel('Duration (days)')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a2c0a-e882-4892-aac8-f2b0a061da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we save the heat extreme start dates and durations in netcdf form\n",
    "\n",
    "event_startdates_xr = xr.DataArray(data=event_startdates).rename('event_startdates')\n",
    "durations_xr = xr.DataArray(data=durations).rename('event_duration')\n",
    "\n",
    "# Create xarray Dataset\n",
    "dataset = xr.Dataset(\n",
    "    {\n",
    "        \"event_startdates\": event_startdates_xr,\n",
    "        \"event_durations\": durations_xr,\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset.to_netcdf(savename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
